import re
import os
from pathlib import Path
import pandas as pd
import shutil
from pypdf import PdfReader
import concurrent.futures
from tqdm import tqdm

# Importa√ß√µes opcionais para OCR (s√≥ se necess√°rio)
try:
    import fitz  # PyMuPDF
    import pytesseract
    from PIL import Image
    import io
    
    # Aumenta o limite de pixels para evitar DecompressionBombWarning em PDFs grandes
    Image.MAX_IMAGE_PIXELS = None
    
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    print("‚ö†Ô∏è  Bibliotecas de OCR n√£o encontradas. OCR ser√° desabilitado.")
    print("   Para habilitar OCR, instale: pip install PyMuPDF pytesseract Pillow")

# --- CONFIGURA√á√ÉO ---
INPUT_DIR = Path(__file__).parent
OUTPUT_STRUCTURED_CSV = 'relatorio_consolidado_extraido.json'
OUTPUT_JUSTIFICATIVAS_CSV = 'relatorio_justificativas.json'
OCR_CACHE_DIR = Path('ocr_cache')
CORRECT_DIR = INPUT_DIR / 'correto'
DEBUG_DIR = INPUT_DIR / 'debug_txt'

# OCR Settings
OCR_LANGUAGE = 'por'
MIN_TEXT_LENGTH = 100  # M√≠nimo de caracteres para considerar que PDF tem texto
TESSERACT_PATH = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# Processing
MAX_WORKERS = os.cpu_count()
# --- FIM DA CONFIGURA√á√ÉO ---

# Configura Tesseract se dispon√≠vel
if OCR_AVAILABLE:
    try:
        pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH
    except:
        pass

# --- CURSOS OFICIAIS DA UFPR ---
# Lista de cursos de gradua√ß√£o (nomes oficiais)
CURSOS_UFPR_OFICIAL = [
    # Curitiba
    'Administra√ß√£o',
    'Agronomia',
    'Arquitetura e Urbanismo',
    'Artes Visuais',
    'Biomedicina',
    'Ci√™ncias Biol√≥gicas',
    'Ci√™ncias da Computa√ß√£o',
    'Ci√™ncias Cont√°beis',
    'Ci√™ncias Econ√¥micas',
    'Ci√™ncias Sociais',
    'Design Gr√°fico',
    'Design de Produto',
    'Direito',
    'Educa√ß√£o F√≠sica',
    'Enfermagem',
    'Engenharia Ambiental',
    'Engenharia de Bioprocessos e Biotecnologia',
    'Engenharia Cartogr√°fica e de Agrimensura',
    'Engenharia Civil',
    'Engenharia El√©trica',
    'Engenharia Florestal',
    'Engenharia Industrial Madereira',
    'Engenharia Mec√¢nica',
    'Engenharia de Produ√ß√£o',
    'Engenharia Qu√≠mica',
    'Estat√≠stica e Ci√™ncia de Dados',
    'Express√£o Gr√°fica',
    'Farm√°cia',
    'Filosofia',
    'F√≠sica',
    'Fisioterapia',
    'Geografia',
    'Geologia',
    'Gest√£o da Informa√ß√£o',
    'Hist√≥ria',
    'Hist√≥ria Mem√≥ria e Imagem',
    'Inform√°tica Biom√©dica',
    'Jornalismo',
    'Letras',
    'Letras Libras',
    'Matem√°tica',
    'Matem√°tica Industrial',
    'Medicina',
    'Medicina Veterin√°ria',
    'M√∫sica',
    'Nutri√ß√£o',
    'Odontologia',
    'Pedagogia',
    'Produ√ß√£o Cultural',
    'Psicologia',
    'Publicidade e Propaganda',
    'Qu√≠mica',
    'Rela√ß√µes P√∫blicas',
    'Tecnologia em An√°lise e Desenvolvimento de Sistemas',
    'Tecnologia em Comunica√ß√£o Institucional',
    'Tecnologia em Gest√£o P√∫blica',
    'Tecnologia em Gest√£o da Qualidade',
    'Tecnologia em Luteria',
    'Tecnologia em Neg√≥cios Imobili√°rios',
    'Tecnologia em Produ√ß√£o C√™nica',
    'Tecnologia em Secretariado',
    'Terapia Ocupacional',
    'Turismo',
    'Zootecnia',
    
    # Jandaia do Sul
    'Ci√™ncias Exatas',  # Licenciatura em Ci√™ncias Exatas (Qu√≠mica, F√≠sica ou Matem√°tica)
    'Engenharia Agr√≠cola',
    'Engenharia de Alimentos',
    'Intelig√™ncia Artificial e Engenharia de Software',
    
    # Matinhos (UFPR Litoral)
    'Administra√ß√£o P√∫blica',
    'Artes',
    'Agroecologia',
    'Ci√™ncias',
    'Ci√™ncias Ambientais',
    'Educa√ß√£o do Campo',
    # 'Educa√ß√£o F√≠sica',  # J√° existe em Curitiba
    # 'Geografia',  # J√° existe em Curitiba
    'Gest√£o de Turismo',
    'Gest√£o e Empreendedorismo',
    'Gest√£o Imobili√°ria',
    'Linguagem e Comunica√ß√£o',
    'Sa√∫de Coletiva',
    'Servi√ßo Social',
    
    # Palotina
    # 'Agronomia',  # J√° existe em Curitiba
    # 'Ci√™ncias Biol√≥gicas',  # J√° existe em Curitiba
    # 'Ci√™ncias Exatas',  # J√° existe em Jandaia do Sul
    'Computa√ß√£o',
    'Engenharia de Aquicultura',
    # 'Engenharia de Bioprocessos e Biotecnologia',  # J√° existe em Curitiba
    'Engenharia de Energias Renov√°veis',
    # 'Medicina Veterin√°ria',  # J√° existe em Curitiba
    
    # Pontal do Paran√° (Centro de Estudos do Mar)
    # 'Ci√™ncias Exatas',  # J√° existe em Jandaia do Sul
    'Engenharia Ambiental e Sanit√°ria',  # Varia√ß√£o de Engenharia Ambiental
    # 'Engenharia Civil',  # J√° existe em Curitiba
    # 'Engenharia de Aquicultura',  # J√° existe em Palotina
    'Oceanografia'
]

# Cria um dicion√°rio para normaliza√ß√£o (todas as varia√ß√µes em UPPER ‚Üí nome oficial)
CURSOS_NORMALIZACAO = {}
for curso in CURSOS_UFPR_OFICIAL:
    # Adiciona o nome oficial em upper

    CURSOS_NORMALIZACAO[curso.upper()] = curso
    # Adiciona varia√ß√µes sem acentos
    curso_sem_acento = (curso.upper()
                        .replace('√Å', 'A').replace('√â', 'E').replace('√ç', 'I')
                        .replace('√ì', 'O').replace('√ö', 'U').replace('√É', 'A')
                        .replace('√ï', 'O').replace('√Ç', 'A').replace('√ä', 'E')
                        .replace('√î', 'O').replace('√á', 'C'))
    CURSOS_NORMALIZACAO[curso_sem_acento] = curso

# Adiciona varia√ß√µes comuns de nomenclatura
CURSOS_NORMALIZACAO.update({
    # Curitiba
    'CIENCIA DA COMPUTACAO': 'Ci√™ncias da Computa√ß√£o',
    'CIENCIAS DA COMPUTACAO': 'Ci√™ncias da Computa√ß√£o',
    'CIENCIA BIOLOGICA': 'Ci√™ncias Biol√≥gicas',
    'CIENCIAS BIOLOGICAS': 'Ci√™ncias Biol√≥gicas',
    'CIENCIA CONTABIL': 'Ci√™ncias Cont√°beis',
    'CIENCIAS CONTABEIS': 'Ci√™ncias Cont√°beis',
    'CIENCIA ECONOMICA': 'Ci√™ncias Econ√¥micas',
    'CIENCIAS ECONOMICAS': 'Ci√™ncias Econ√¥micas',
    'CIENCIA SOCIAL': 'Ci√™ncias Sociais',
    'DESIGN GRAFICO': 'Design Gr√°fico',
    'EDUCACAO FISICA': 'Educa√ß√£o F√≠sica',
    'ENGENHARIA ELETRICA': 'Engenharia El√©trica',
    'ESTATISTICA E CIENCIA DE DADOS': 'Estat√≠stica e Ci√™ncia de Dados',
    'EXPRESSAO GRAFICA': 'Express√£o Gr√°fica',
    'FARMACIA': 'Farm√°cia',
    'FISICA': 'F√≠sica',
    'HISTORIA': 'Hist√≥ria',
    'HISTORIA MEMORIA E IMAGEM': 'Hist√≥ria Mem√≥ria e Imagem',
    'INFORMATICA BIOMEDICA': 'Inform√°tica Biom√©dica',
    'MATEMATICA': 'Matem√°tica',
    'MATEMATICA INDUSTRIAL': 'Matem√°tica Industrial',
    'MEDICINA VETERINARIA': 'Medicina Veterin√°ria',
    'MUSICA': 'M√∫sica',
    'NUTRICAO': 'Nutri√ß√£o',
    'QUIMICA': 'Qu√≠mica',
    'RELACOES PUBLICAS': 'Rela√ß√µes P√∫blicas',
    'TERAPIA OCUPACIONAL': 'Terapia Ocupacional',
    'ANALISE E DESENVOLVIMENTO DE SISTEMAS': 'Tecnologia em An√°lise e Desenvolvimento de Sistemas',
    'COMUNICACAO INSTITUCIONAL': 'Tecnologia em Comunica√ß√£o Institucional',
    'GESTAO PUBLICA': 'Tecnologia em Gest√£o P√∫blica',
    'GESTAO DA QUALIDADE': 'Tecnologia em Gest√£o da Qualidade',
    'LUTERIA': 'Tecnologia em Luteria',
    'NEGOCIOS IMOBILIARIOS': 'Tecnologia em Neg√≥cios Imobili√°rios',
    'PRODUCAO CENICA': 'Tecnologia em Produ√ß√£o C√™nica',
    'SECRETARIADO': 'Tecnologia em Secretariado',
    
    # Jandaia do Sul
    'CIENCIAS EXATAS': 'Ci√™ncias Exatas',
    'LICENCIATURA EM CIENCIAS EXATAS': 'Ci√™ncias Exatas',
    'LICENCIATURA CIENCIAS EXATAS': 'Ci√™ncias Exatas',
    'CIENCIAS EXATAS - QUIMICA, FISICA OU MATEMATICA': 'Ci√™ncias Exatas',
    'CIENCIAS EXATAS QUIMICA FISICA OU MATEMATICA': 'Ci√™ncias Exatas',
    'ENGENHARIA AGRICOLA': 'Engenharia Agr√≠cola',
    'ENGENHARIA DE ALIMENTOS': 'Engenharia de Alimentos',
    'INTELIGENCIA ARTIFICIAL E ENGENHARIA DE SOFTWARE': 'Intelig√™ncia Artificial e Engenharia de Software',
    'INTELIGENCIA ARTIFICIAL': 'Intelig√™ncia Artificial e Engenharia de Software',
    'ENGENHARIA DE SOFTWARE': 'Intelig√™ncia Artificial e Engenharia de Software',
    'IA E ENGENHARIA DE SOFTWARE': 'Intelig√™ncia Artificial e Engenharia de Software',
    
    # Matinhos (UFPR Litoral)
    'ADMINISTRACAO PUBLICA': 'Administra√ß√£o P√∫blica',
    'ADMINISTRACAO PUBLICA': 'Administra√ß√£o P√∫blica',
    'AGROECOLOGIA': 'Agroecologia',
    'CIENCIAS AMBIENTAIS': 'Ci√™ncias Ambientais',
    'CIENCIA AMBIENTAL': 'Ci√™ncias Ambientais',
    'EDUCACAO DO CAMPO': 'Educa√ß√£o do Campo',
    'GESTAO DE TURISMO': 'Gest√£o de Turismo',
    'GESTAO DO TURISMO': 'Gest√£o de Turismo',
    'GESTAO E EMPREENDEDORISMO': 'Gest√£o e Empreendedorismo',
    'GESTAO IMOBILIARIA': 'Gest√£o Imobili√°ria',
    'GESTAO IMOBILIARIA': 'Gest√£o Imobili√°ria',
    'LINGUAGEM E COMUNICACAO': 'Linguagem e Comunica√ß√£o',
    'SAUDE COLETIVA': 'Sa√∫de Coletiva',
    'SERVICO SOCIAL': 'Servi√ßo Social',
    
    # Palotina
    'COMPUTACAO': 'Computa√ß√£o',
    'ENGENHARIA DE AQUICULTURA': 'Engenharia de Aquicultura',
    'AQUICULTURA': 'Engenharia de Aquicultura',
    'ENGENHARIA DE ENERGIAS RENOVAVEIS': 'Engenharia de Energias Renov√°veis',
    'ENGENHARIA DE ENERGIAS RENOVAVEIS': 'Engenharia de Energias Renov√°veis',
    'ENERGIAS RENOVAVEIS': 'Engenharia de Energias Renov√°veis',
    
    # Pontal do Paran√° (Centro de Estudos do Mar)
    'ENGENHARIA AMBIENTAL E SANITARIA': 'Engenharia Ambiental e Sanit√°ria',
    'ENGENHARIA AMBIENTAL E SANITARIA': 'Engenharia Ambiental e Sanit√°ria',
    'AMBIENTAL E SANITARIA': 'Engenharia Ambiental e Sanit√°ria',
    'OCEANOGRAFIA': 'Oceanografia',
})



# --- PADR√ïES DE REGEX ---
# Para dados estruturados - Padr√µes MUITO mais estritos

# Nome do curso - M√∫ltiplos padr√µes
# Padr√£o 1: Do cabe√ßalho estruturado "Curso(s) / Habilita√ß√£o(√µes) sendo avaliado(s):"
NOME_CURSO_HEADER_PATTERN = re.compile(
    r"Curso\(s\)\s*/\s*Habilita√ß√£o\(√µes\)\s+sendo\s+avaliado\(s\)[:\s]+([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á][A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√áa-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß\s-]+?)\s*(?:Informa√ß√µes|$)",
    re.IGNORECASE
)
# Padr√£o 2: Do corpo do documento
NOME_CURSO_PATTERN = re.compile(
    r"Curso\(s\)\s*/\s*Habilita√ß√£o\(√µes\)[^:]*?:\s*([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á][A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√áa-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß\s-]+?)\s*;\s*Grau",
    re.IGNORECASE
)

# C√≥digo MEC - M√∫ltiplos padr√µes
# Padr√£o 1: "C√≥digo MEC:"
CODIGO_MEC_HEADER_PATTERN = re.compile(r"C√≥digo\s+MEC[:\s]+(\d{6,8})", re.IGNORECASE)
# Padr√£o 2: "C√≥digo do Curso" ou "C√≥digo e-MEC do Curso"
CODIGO_MEC_PATTERN = re.compile(r"C√≥digo\s+(?:e-MEC\s+)?do\s+Curso[:\s]+(\d{6,8})", re.IGNORECASE)

# Ano de avalia√ß√£o - M√∫ltiplos padr√µes
# Padr√£o 1: "Per√≠odo de Visita: DD/MM/AAAA"
ANO_VISITA_PATTERN = re.compile(
    r"Per√≠odo\s+de\s+Visita[:\s]+\d{1,2}/\d{1,2}/(20\d{2})",
    re.IGNORECASE
)
# Padr√£o 2: "ocorreu no per√≠odo"
ANO_AVALIACAO_PATTERN = re.compile(
    r"ocorreu\s+no\s+per[√≠i]odo.*?(20\d{2})",
    re.IGNORECASE | re.DOTALL
)

# Cidade - Extrai ap√≥s CEP no formato "Cidade - UF"
CIDADE_PATTERN = re.compile(
    r"CEP[:\s-]*(\d[\d.-]+).*?([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á][a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß]+(?:\s+[a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß]+)?)[\s-]+[A-Z]{2}\.",
    re.IGNORECASE | re.DOTALL
)

# Campus - M√∫ltiplos padr√µes
# Padr√£o 1: Do endere√ßo (ex: "campus centro")
CAMPUS_HEADER_PATTERN = re.compile(
    r"\d+\s*-\s*campus\s+([a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß\s]+?)\s*-",
    re.IGNORECASE
)
# Padr√£o 2 : Do texto "Campus X, situado"
CAMPUS_PATTERN = re.compile(
    r"Campus\s+([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á][a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß]+(?:\s+[A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á][a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß]+){0,3}),\s*situado",
    re.IGNORECASE
)

# Modalidade - APENAS ap√≥s Grau:
MODALIDADE_PATTERN = re.compile(
    r"Grau:\s*(Licenciatura|Bacharelado|Tecn√≥logo)",
    re.IGNORECASE
)

# Padr√µes para conceitos
CONCEITOS_PATTERN = re.compile(
    r"CONCEITO\s+FINAL\s+CONT√çNUO\s+CONCEITO\s+FINAL\s+FAIXA\s*([\d,.]+)\s+([\d,.]+)",
    re.IGNORECASE | re.DOTALL
)

# Padr√£o para indicadores
INDICADORES_PATTERN = re.compile(
    r"^\s*(\d+\.\d+)\..*?Justificativa para conceito\s*(.*?)\s*:",
    re.MULTILINE | re.DOTALL | re.IGNORECASE
)

# Padr√£o para dimens√µes (notas globais das 3 dimens√µes)
# Captura apenas valores no formato X,XX ou X.XX (entre 1-5)
DIMENSOES_PATTERN = re.compile(
    r"Dimens√£o\s+(\d):\s*[^\d]*?([1-5][\.,]\d{1,2}|[1-5])",
    re.IGNORECASE
)

# Mapeamento Campus ‚Üí Cidade (oficial UFPR)
CAMPUS_CIDADE_MAP = {
    # Curitiba
    'Pr√©dio Hist√≥rico': 'Curitiba',
    'Reitoria': 'Curitiba',
    'Rebou√ßas': 'Curitiba',
    'Batel': 'Curitiba',
    'Juvev√™': 'Curitiba',
    'Cabral': 'Curitiba',
    'Jardim Bot√¢nico': 'Curitiba',
    'Centro Polit√©cnico': 'Curitiba',
    'Complexo Hospital de Cl√≠nicas': 'Curitiba',
    'Setor de Educa√ß√£o Profissional e Tecnol√≥gica (SEPT)': 'Curitiba',
    
    # Pontal do Paran√°
    'Centro de Estudos do Mar': 'Pontal do Paran√°',
    
    # Matinhos
    'UFPR Litoral': 'Matinhos',
    
    # Palotina
    'Campus Palotina': 'Palotina',
    
    # Jandaia do Sul
    'Campus Avan√ßado Jandaia do Sul': 'Jandaia do Sul',
    
    # Toledo
    'Campus Toledo': 'Toledo'
}

# Mapeamento de campus da UFPR (para normaliza√ß√£o de texto extra√≠do ‚Üí nome oficial)
CAMPUS_UFPR = {
    # Curitiba
    'PR√âDIO HIST√ìRICO': 'Pr√©dio Hist√≥rico',
    'PREDIO HISTORICO': 'Pr√©dio Hist√≥rico',
    'HIST√ìRICO': 'Pr√©dio Hist√≥rico',
    'HISTORICO': 'Pr√©dio Hist√≥rico',
    'REITORIA': 'Reitoria',
    'REBOU√áAS': 'Rebou√ßas',
    'REBOUCAS': 'Rebou√ßas',
    'BATEL': 'Batel',
    'JUVEV√ä': 'Juvev√™',
    'JUVEVE': 'Juvev√™',
    'CABRAL': 'Cabral',
    'JARDIM BOT√ÇNICO': 'Jardim Bot√¢nico',
    'JARDIM BOTANICO': 'Jardim Bot√¢nico',
    'CENTRO POLIT√âCNICO': 'Centro Polit√©cnico',
    'CENTRO POLITECNICO': 'Centro Polit√©cnico',
    'POLIT√âCNICO': 'Centro Polit√©cnico',
    'POLITECNICO': 'Centro Polit√©cnico',
    'HOSPITAL DE CL√çNICAS': 'Complexo Hospital de Cl√≠nicas',
    'HOSPITAL DE CLINICAS': 'Complexo Hospital de Cl√≠nicas',
    'COMPLEXO HOSPITAL DE CL√çNICAS': 'Complexo Hospital de Cl√≠nicas',
    'COMPLEXO HOSPITAL DE CLINICAS': 'Complexo Hospital de Cl√≠nicas',
    'HC': 'Complexo Hospital de Cl√≠nicas',
    'SEPT': 'Setor de Educa√ß√£o Profissional e Tecnol√≥gica (SEPT)',
    'SETOR DE EDUCA√á√ÉO PROFISSIONAL E TECNOL√ìGICA': 'Setor de Educa√ß√£o Profissional e Tecnol√≥gica (SEPT)',
    'SETOR DE EDUCACAO PROFISSIONAL E TECNOLOGICA': 'Setor de Educa√ß√£o Profissional e Tecnol√≥gica (SEPT)',
    
    # Pontal do Paran√°
    'PONTAL DO PARAN√Å': 'Centro de Estudos do Mar',
    'PONTAL DO PARANA': 'Centro de Estudos do Mar',
    'PONTAL': 'Centro de Estudos do Mar',
    'CENTRO DE ESTUDOS DO MAR': 'Centro de Estudos do Mar',
    'CEM': 'Centro de Estudos do Mar',
    
    # Matinhos
    'MATINHOS': 'UFPR Litoral',
    'LITORAL': 'UFPR Litoral',
    'UFPR LITORAL': 'UFPR Litoral',
    
    # Palotina
    'PALOTINA': 'Campus Palotina',
    'CAMPUS PALOTINA': 'Campus Palotina',
    
    # Jandaia do Sul
    'JANDAIA DO SUL': 'Campus Avan√ßado Jandaia do Sul',
    'JANDAIA': 'Campus Avan√ßado Jandaia do Sul',
    'CAMPUS AVAN√áADO JANDAIA DO SUL': 'Campus Avan√ßado Jandaia do Sul',
    'CAMPUS AVANCADO JANDAIA DO SUL': 'Campus Avan√ßado Jandaia do Sul',
    
    # Toledo
    'TOLEDO': 'Campus Toledo',
    'CAMPUS TOLEDO': 'Campus Toledo'
}

def normalizar_campus(campus_extraido):
    """
    Normaliza o nome do campus extra√≠do para o nome oficial da UFPR.
    """
    if not campus_extraido:
        return ''
    
    campus_upper = campus_extraido.strip().upper()
    
    # Busca exata no mapeamento
    if campus_upper in CAMPUS_UFPR:
        return CAMPUS_UFPR[campus_upper]
    
    # Busca parcial (cont√©m)
    for chave, valor in CAMPUS_UFPR.items():
        if chave in campus_upper or campus_upper in chave:
            return valor
    
    # Se n√£o encontrou, retorna o original
    return campus_extraido.strip()


def normalizar_cidade(cidade_extraida):
    """
    Normaliza o nome da cidade extra√≠da.
    """
    if not cidade_extraida:
        return ''
    
    # Capitaliza corretamente
    cidade = cidade_extraida.strip().title()
    return cidade


def normalizar_modalidade(modalidade_extraida):
    """
    Normaliza a modalidade do curso.
    """
    if not modalidade_extraida:
        return ''
    
    modalidade_map = {
        'LICENCIATURA': 'Licenciatura',
        'BACHARELADO': 'Bacharelado',
        'TECN√ìLOGO': 'Tecn√≥logo',
        'TECNOLOGO': 'Tecn√≥logo'
    }
    
    modalidade_upper = modalidade_extraida.strip().upper()
    return modalidade_map.get(modalidade_upper, modalidade_extraida.strip())


def formatar_decimal(valor):
    """
    Converte um valor para float.
    Retorna float, 'NSA' ou None se n√£o for v√°lido.
    """
    if not valor:
        return None
    
    valor_str = str(valor).strip().upper()
    
    # Se for NSA, mant√©m como est√°
    if valor_str == 'NSA':
        return 'NSA'
    
    try:
        # Remove v√≠rgulas e converte para float
        return float(valor_str.replace(',', '.'))
    except (ValueError, TypeError):
        # Se n√£o for poss√≠vel converter, retorna None
        return None


def formatar_inteiro(valor):
    """
    Converte um valor para int.
    Retorna int, 'NSA' ou None se n√£o for v√°lido.
    """
    if not valor:
        return None
    
    valor_str = str(valor).strip().upper()
    
    # Se for NSA, mant√©m como est√°
    if valor_str == 'NSA':
        return 'NSA'
    
    try:
        # Remove v√≠rgulas e converte para float antes de int (para lidar com "2019.0")
        return int(float(valor_str.replace(',', '.')))
    except (ValueError, TypeError):
        # Se n√£o for poss√≠vel converter, retorna None
        return None


def normalizar_curso(curso_extraido):
    """
    Normaliza o nome do curso extra√≠do para o nome oficial da UFPR.
    Retorna o nome oficial se encontrado, ou o nome original se n√£o encontrar.
    """
    if not curso_extraido:
        return ''
    
    curso_upper = curso_extraido.strip().upper()
    
    # Remove prefixos de modalidade que podem ter sido capturados
    curso_upper = re.sub(r'^(BACHARELADO|LICENCIATURA|TECN√ìLOGO|TECNOLOGO)\s+(EM\s+)?', '', curso_upper)
    
    # Busca exata no dicion√°rio de normaliza√ß√£o
    if curso_upper in CURSOS_NORMALIZACAO:
        return CURSOS_NORMALIZACAO[curso_upper]
    
    # Busca parcial - verifica se algum curso conhecido est√° contido no texto extra√≠do
    for variacao, curso_oficial in CURSOS_NORMALIZACAO.items():
        if variacao in curso_upper or curso_upper in variacao:
            # Verifica se a similaridade √© alta o suficiente
            if len(curso_upper) >= 5 and len(variacao) >= 5:
                return curso_oficial
    
    # Se n√£o encontrou correspond√™ncia, retorna o original (title case)
    return curso_extraido.strip().title()


# --- FIM DOS PADR√ïES ---


def verificar_texto_pdf(pdf_path: Path) -> bool:
    """
    Verifica se o PDF tem texto extra√≠vel suficiente.
    Retorna True se tiver texto, False caso contr√°rio.
    """
    try:
        reader = PdfReader(pdf_path)
        # Verifica as primeiras 3 p√°ginas
        texto_total = ""
        for i, page in enumerate(reader.pages[:3]):
            texto_total += page.extract_text() or ""
            if len(texto_total) > MIN_TEXT_LENGTH:
                return True
        
        return len(texto_total.strip()) > MIN_TEXT_LENGTH
    except Exception as e:
        print(f"‚ùå Erro ao verificar texto em '{pdf_path.name}': {e}")
        return False


def aplicar_ocr_pdf(pdf_path: Path) -> Path:
    """
    Aplica OCR a um PDF e salva a vers√£o com OCR no cache.
    Retorna o caminho do PDF com OCR.
    """
    if not OCR_AVAILABLE:
        raise RuntimeError("OCR n√£o est√° dispon√≠vel. Instale as bibliotecas necess√°rias.")
    
    # Cria diret√≥rio de cache se n√£o existir
    OCR_CACHE_DIR.mkdir(exist_ok=True)
    
    output_pdf_path = OCR_CACHE_DIR / pdf_path.name
    
    # Se j√° foi processado, retorna o cache
    if output_pdf_path.exists():
        print(f"  ‚ôªÔ∏è  Usando PDF com OCR do cache: {pdf_path.name}")
        return output_pdf_path
    
    print(f"  üîç Aplicando OCR em: {pdf_path.name}")
    
    try:
        doc = fitz.open(pdf_path)
        output_pdf = fitz.open()
        
        for i, page in enumerate(doc):
            # Tenta renderizar com diferentes DPIs para evitar erro de mem√≥ria
            dpis_to_try = [300, 150, 72]
            pix = None
            
            for dpi in dpis_to_try:
                try:
                    pix = page.get_pixmap(dpi=dpi)
                    break # Sucesso
                except RuntimeError as e:
                    print(f"  [!] Erro de mem√≥ria com DPI {dpi} na p√°g {i+1}. Tentando menor...")
                    if dpi == dpis_to_try[-1]:
                        raise e # Se falhar no menor, relan√ßa
            
            if not pix:
                continue
                
            img = Image.open(io.BytesIO(pix.tobytes("png")))
            
            print(f"    ... OCR na p√°gina {i+1}/{len(doc)} (DPI: {dpi})")
            
            # Aplica OCR com timeout de 2 minutos por p√°gina
            try:
                pagina_com_ocr_bytes = pytesseract.image_to_pdf_or_hocr(
                    img, lang=OCR_LANGUAGE, extension='pdf', timeout=120
                )
                
                pagina_com_ocr = fitz.open("pdf", pagina_com_ocr_bytes)
                output_pdf.insert_pdf(pagina_com_ocr)
            except RuntimeError as e:
                print(f"    [!] Timeout ou erro no Tesseract na p√°gina {i+1}: {e}")
                # Opcional: Adicionar a p√°gina original sem OCR se falhar?
                # Por enquanto, apenas pula a p√°gina problem√°tica para n√£o travar tudo
                continue
        
        output_pdf.save(output_pdf_path, garbage=4, deflate=True, clean=True)
        output_pdf.close()
        doc.close()
        
        print(f"  ‚úÖ OCR conclu√≠do: {output_pdf_path.name}")
        return output_pdf_path
        
    except Exception as e:
        # Se houver erro fatal no OCR, retorna o original para n√£o parar o fluxo, mas avisa
        print(f"‚ùå Erro cr√≠tico no OCR de '{pdf_path.name}': {e}")
        return pdf_path


def extrair_texto_pdf(pdf_path: Path, usar_ocr: bool = False) -> str:
    """
    Extrai texto de um PDF.
    Se usar_ocr=True, aplica OCR primeiro.
    """
    try:
        if usar_ocr:
            pdf_path = aplicar_ocr_pdf(pdf_path)
        
        reader = PdfReader(pdf_path)
        texto_completo = ""
        for page in reader.pages:
            texto_completo += page.extract_text() or ""
            texto_completo += "\n"
        
        return texto_completo
        
    except Exception as e:
        print(f"‚ùå Erro ao extrair texto de '{pdf_path.name}': {e}")
        return ""


def extrair_dados_estruturados(texto: str, pdf_path: Path) -> dict:
    """
    Extrai dados estruturados do texto (curso, indicadores, conceitos).
    """
    dados = {}
    
    # Limpeza b√°sica: normaliza espa√ßos
    texto_limpo = re.sub(r'\s+', ' ', texto).strip()
    
    # --- Extra√ß√£o do Curso e C√≥digo ---
    # TENTA o padr√£o do cabe√ßalho PRIMEIRO
    nome_curso_match = NOME_CURSO_HEADER_PATTERN.search(texto_limpo)
    if not nome_curso_match:
        # Se n√£o encontrou, tenta o padr√£o do corpo do documento
        nome_curso_match = NOME_CURSO_PATTERN.search(texto_limpo)
    
    if nome_curso_match:
        nome_curso = nome_curso_match.group(1).strip()
        
        # Valida√ß√£o RIGOROSA - verifica palavras inv√°lidas
        palavras_invalidas = ['informa√ß√µes', 'comiss√£o', 'avalia√ß√£o', 'regula√ß√£o', 'docentes', 
                               'categorias', 'processo seletivo', 'vestibular', 'prof', 'dra', 
                               'quest√µes', 'atendimento', 'regime', 'lei', 'decreto', 'ci√™ncias jur√≠dicas']
        
        # Verifica se √© um nome v√°lido (n√£o √© s√≥ n√∫meros nem cont√©m palavras inv√°lidas)
        if (len(nome_curso) <= 80 and len(nome_curso) >= 3 and 
            not nome_curso.isdigit() and 
            not any(inv in nome_curso.lower() for inv in palavras_invalidas) and
            re.search(r'[A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á]', nome_curso)):  # Deve ter pelo menos uma letra mai√∫scula
            
            # Normaliza o nome do curso usando a lista oficial
            curso_normalizado = normalizar_curso(nome_curso)
            dados['Curso'] = curso_normalizado
        else:
            dados['Curso'] = None
    else:
        dados['Curso'] = None
    
    # C√≥digo MEC: tenta padr√£o do header primeiro
    codigo_match = CODIGO_MEC_HEADER_PATTERN.search(texto_limpo)
    if not codigo_match:
        codigo_match = CODIGO_MEC_PATTERN.search(texto_limpo)
    
    dados['Id_MEC'] = formatar_inteiro(codigo_match.group(1).strip()) if codigo_match else None
    
    # --- Extra√ß√£o de Ano, Cidade, Campus, Modalidade ---
    # Ano de avalia√ß√£o: tenta padr√£o de visita primeiro
    ano_match = ANO_VISITA_PATTERN.search(texto_limpo)
    if not ano_match:
        ano_match = ANO_AVALIACAO_PATTERN.search(texto_limpo)
    
    if ano_match:
        ano = ano_match.group(1).strip()
        # Valida que √© um ano entre 2000-2099
        if ano.isdigit() and 2000 <= int(ano) <= 2099:
            dados['Ano_avaliacao'] = formatar_inteiro(ano)
        else:
            dados['Ano_avaliacao'] = None
    else:
        dados['Ano_avaliacao'] = None
    
    # Campus: tenta padr√£o do header primeiro
    campus_match = CAMPUS_HEADER_PATTERN.search(texto_limpo)
    if not campus_match:
        campus_match = CAMPUS_PATTERN.search(texto_limpo)
    
    if campus_match:
        campus_bruto = campus_match.group(1).strip()
        # Normaliza usando a lista v√°lida (j√° valida automaticamente)
        campus_normalizado = normalizar_campus(campus_bruto)
        dados['Campus'] = campus_normalizado
        
        # Preenche a Cidade automaticamente baseado no Campus normalizado
        dados['Cidade'] = CAMPUS_CIDADE_MAP.get(campus_normalizado, None)
    else:
        dados['Campus'] = None
        # Se n√£o encontrou campus, tenta extrair cidade diretamente do texto
        cidade_match = CIDADE_PATTERN.search(texto_limpo)
        if cidade_match:
            cidade_bruta = cidade_match.group(2).strip()  # Grupo 2 √© a cidade
            # Remove preposi√ß√µes comuns
            cidade_bruta = re.sub(r'^(de|da|do|dos|das)\s+', '', cidade_bruta, flags=re.IGNORECASE)
            dados['Cidade'] = cidade_bruta.strip().title()
        else:
            dados['Cidade'] = None
    
    # Modalidade - extrai e normaliza
    modalidade_match = MODALIDADE_PATTERN.search(texto_limpo)
    if modalidade_match:
        modalidade_bruta = modalidade_match.group(1)
        # Normaliza usando a lista v√°lida
        dados['Modalidade'] = normalizar_modalidade(modalidade_bruta)
    else:
        dados['Modalidade'] = None

    
    # --- Extra√ß√£o de Dimens√µes e Indicadores (Valores Relacionados) ---
    
    # 1. Inicializa√ß√£o
    # Dimens√µes
    dados['1'] = None
    dados['2'] = None
    dados['3'] = None
    
    # Indicadores
    indicadores_esperados = []
    indicadores_esperados.extend([f"1.{i}" for i in range(1, 25)]) # Dimens√£o 1
    indicadores_esperados.extend([f"2.{i}" for i in range(1, 17)]) # Dimens√£o 2
    indicadores_esperados.extend([f"3.{i}" for i in range(1, 18)]) # Dimens√£o 3
    
    for ind in indicadores_esperados:
        dados[ind] = None

    # 2. Extra√ß√£o de Dimens√µes
    dimensoes_matches = DIMENSOES_PATTERN.findall(texto_limpo)
    for num_dim, nota_dim in dimensoes_matches:
        valor_formatado = formatar_decimal(nota_dim)
        if valor_formatado is not None and valor_formatado != 'NSA':
            if 1.0 <= valor_formatado <= 5.0:
                dados[num_dim] = valor_formatado
            else:
                print(f"  ‚ö†Ô∏è  Valor inv√°lido para dimens√£o {num_dim}: {valor_formatado} (esperado: 1-5)")
        else:
            dados[num_dim] = valor_formatado

    # 3. Extra√ß√£o de Indicadores
    matches = INDICADORES_PATTERN.findall(texto)
    for indicador, nota in matches:
        if indicador in indicadores_esperados:
            dados[indicador] = formatar_decimal(nota)
    
    # --- Extra√ß√£o de Conceitos ---
    conceitos_match = CONCEITOS_PATTERN.search(texto_limpo)
    if conceitos_match:
        dados['CONCEITO FINAL CONT√çNUO'] = formatar_decimal(conceitos_match.group(1).strip())
        dados['CONCEITO FINAL FAIXA'] = formatar_inteiro(conceitos_match.group(2).strip())
    else:
        dados['CONCEITO FINAL CONT√çNUO'] = None
        dados['CONCEITO FINAL FAIXA'] = None
    
    return dados


def extrair_justificativas(texto: str, curso_id: str) -> list:
    """
    Extrai justificativas de indicadores do texto.
    Retorna lista de dicts com CURSO, INDICADOR, JUSTIFICATIVA.
    """
    lista_justificativas = []
    
    # Divide o texto em blocos por indicador
    blocos = re.split(r'(?=^\s*\d+\.\d+\.)', texto, flags=re.MULTILINE)
    
    for bloco in blocos:
        # Extrai n√∫mero do indicador
        indicador_match = re.match(r'^\s*(\d+\.\d+)\.', bloco)
        if not indicador_match:
            continue
        
        indicador_num = indicador_match.group(1)
        
        # Extrai justificativa
        justificativa_match = re.search(
            r"Justificativa para conceito.*?:(.*?)(?=\Z)",
            bloco, re.IGNORECASE | re.DOTALL
        )
        
        if justificativa_match:
            justificativa_texto = ' '.join(justificativa_match.group(1).split())
            
            lista_justificativas.append({
                'ID_DOCUMENTO': curso_id,  # Agora recebe o ID_DOCUMENTO (nome do arquivo)
                'INDICADOR': indicador_num,
                'JUSTIFICATIVA': justificativa_texto
            })
    
    return lista_justificativas


def processar_um_pdf(pdf_path: Path) -> tuple:
    """
    Processa um √∫nico PDF.
    Retorna tupla: (dados_estruturados, lista_justificativas)
    """
    print(f"\n[>] Processando: {pdf_path.name}")
    
    # Verifica se tem texto nativo
    tem_texto = verificar_texto_pdf(pdf_path)
    
    usar_ocr = False
    if not tem_texto:
        if OCR_AVAILABLE:
            print(f"  [!] PDF sem texto detectado. Usando OCR.")
            usar_ocr = True
        else:
            print(f"  [X] PDF sem texto e OCR n√£o dispon√≠vel. Pulando arquivo.")
            return None, None
    else:
        print(f"  [i] PDF com texto detectado. Usando extra√ß√£o nativa.")
    
    # Extrai texto (Nativo ou OCR)
    texto = extrair_texto_pdf(pdf_path, usar_ocr=usar_ocr)
    
    # Salva o texto extra√≠do para debug
    try:
        debug_file = DEBUG_DIR / f"{pdf_path.stem}.txt"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(texto if texto else "")
    except Exception as e:
        print(f"  [!] Erro ao salvar txt de debug: {e}")
    
    if not texto:
        print(f"  [X] Falha na extra√ß√£o de texto.")
        return None, None
        
    # Extra√ß√£o de dados estruturados
    dados = extrair_dados_estruturados(texto, pdf_path)
    
    # Adiciona ID √∫nico baseado no nome do arquivo para relacionamento
    id_documento = pdf_path.name
    dados['ID_DOCUMENTO'] = id_documento
    
    # Extra√ß√£o de justificativas (usando ID_DOCUMENTO como chave estrangeira)
    justificativas = extrair_justificativas(texto, id_documento)
    
    # Enriquece justificativas com dados do curso para facilitar relacionamento
    for just in justificativas:
        just['Curso'] = dados.get('Curso')
        just['Id_MEC'] = dados.get('Id_MEC')
    
    # Crit√©rio de sucesso: Todos os campos principais foram extra√≠dos
    campos_obrigatorios = ['Curso', 'Id_MEC', 'Ano_avaliacao', 'Modalidade', 'Cidade', 'Campus']
    dados_completos = all(dados.get(campo) is not None for campo in campos_obrigatorios)
    
    if dados_completos:
        print(f"  [OK] Sucesso (Todos os campos principais extra√≠dos)")
        print(f"  [OK] Extra√≠do: {len(justificativas)} justificativas")
        
        # Move arquivo para pasta de corretos
        try:
            shutil.move(str(pdf_path), str(CORRECT_DIR / pdf_path.name))
            print(f"  [->] Arquivo movido para: {CORRECT_DIR.name}")
        except Exception as e:
            print(f"  [!] Erro ao mover arquivo: {e}")
            
        return dados, justificativas
    else:
        campos_faltantes = [campo for campo in campos_obrigatorios if dados.get(campo) is None]
        print(f"  [!] Dados incompletos. Faltando: {campos_faltantes}")
        return dados, justificativas


def processar_todos_pdfs():
    """
    Processa todos os PDFs e gera os CSVs.
    """
    pdf_files = list(INPUT_DIR.glob('*.pdf'))
    
    if not pdf_files:
        print(f"[X] Nenhum PDF encontrado em '{INPUT_DIR.resolve()}'")
        return
    
    # Cria diret√≥rio de corretos se n√£o existir
    CORRECT_DIR.mkdir(exist_ok=True)
    # Cria diret√≥rio de debug se n√£o existir
    DEBUG_DIR.mkdir(exist_ok=True)
    
    print(f"\n[*] Encontrados {len(pdf_files)} PDFs para processar")
    print(f"[*] Usando at√© {MAX_WORKERS} processos paralelos\n")
    
    # Listas para armazenar resultados
    lista_dados_estruturados = []
    lista_justificativas_completa = []
    
    # Processamento paralelo
    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        resultados = list(tqdm(
            executor.map(processar_um_pdf, pdf_files),
            total=len(pdf_files),
            desc="[*] Progresso",
            unit="PDF"
        ))
    
    # Consolida resultados
    for dados, justificativas in resultados:
        if dados:
            lista_dados_estruturados.append(dados)
        if justificativas:
            lista_justificativas_completa.extend(justificativas)
    
    # Gera CSV de dados estruturados
    if lista_dados_estruturados:
        df_estruturados = pd.DataFrame(lista_dados_estruturados)
        
        # Define ordem das colunas
        fixed_cols_start = ['ID_DOCUMENTO', 'Curso', 'Id_MEC', 'Ano_avaliacao', 'Modalidade', 'Cidade', 'Campus']
        ind_1 = [f'1.{i}' for i in range(1, 25)]
        ind_2 = [f'2.{i}' for i in range(1, 17)]
        ind_3 = [f'3.{i}' for i in range(1, 18)]
        dimensoes = ['1', '2', '3']  # Notas das dimens√µes
        fixed_cols_end = ['CONCEITO FINAL CONT√çNUO', 'CONCEITO FINAL FAIXA']
        
        all_cols = fixed_cols_start + ind_1 + ind_2 + ind_3 + dimensoes + fixed_cols_end
        
        # Garante que colunas inteiras n√£o sejam convertidas para float (devido a NaNs)
        cols_inteiras = ['Id_MEC', 'Ano_avaliacao', 'CONCEITO FINAL FAIXA']
        for col in cols_inteiras:
            if col in df_estruturados.columns:
                df_estruturados[col] = df_estruturados[col].astype(object).where(df_estruturados[col].notnull(), None)
        
        for col in all_cols:
            if col not in df_estruturados.columns:
                df_estruturados[col] = None
        
        df_estruturados = df_estruturados.reindex(columns=all_cols)
        df_estruturados.to_json(OUTPUT_STRUCTURED_CSV, orient='records', force_ascii=False, indent=2)
        
        print(f"\n[OK] Dados estruturados salvos: '{OUTPUT_STRUCTURED_CSV}'")
        print(f"   [*] {len(df_estruturados)} cursos processados")
    else:
        print("\n[!] Nenhum dado estruturado extra√≠do")
    
    # Gera CSV de justificativas
    if lista_justificativas_completa:
        df_justificativas = pd.DataFrame(lista_justificativas_completa)
        
        # Reordena colunas para ficar mais organizado
        cols_order = ['ID_DOCUMENTO', 'Curso', 'Id_MEC', 'INDICADOR', 'JUSTIFICATIVA']
        # Garante que todas as colunas existam
        for col in cols_order:
            if col not in df_justificativas.columns:
                df_justificativas[col] = None
                
        df_justificativas = df_justificativas.reindex(columns=cols_order)
        
        df_justificativas.to_json(OUTPUT_JUSTIFICATIVAS_CSV, orient='records', force_ascii=False, indent=2)
        
        print(f"[OK] Justificativas salvas: '{OUTPUT_JUSTIFICATIVAS_CSV}'")
        print(f"   [*] {len(df_justificativas)} justificativas extra√≠das")
    else:
        print("\n[!] Nenhuma justificativa extra√≠da")
    
    print("\n[*] Processamento conclu√≠do!")


if __name__ == '__main__':
    processar_todos_pdfs()
